---
title: "R Notebook"
output: html_notebook
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code.

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Cmd+Shift+Enter*.

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Cmd+Option+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Cmd+Shift+K* to preview the HTML file).

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.

```{r}
rm(list=objects())

library(parallel)
library(lightgbm)
library(xgboost)
library(Metrics)
# Load required libraries
library(dplyr)
library(data.table)
library(caret)
library(zoo)
library(data.table)
library(lightgbm)
library(xgboost)
library(tidyverse)
library(glmnet)
library(forecast)
library(mgcv)
library(mgcViz)
library(gridExtra)
library(yarrr)
library(lightgbm)
library(xgboost)
library(dplyr)
library(tidyr)
library(dygraphs)
library(xts)
library(tidyverse)
library(ggplot2)

```

```{r}
reduceMemUsage <- function(df, verbose = FALSE) {
  # Initial memory usage calculation
  start_mem <- sum(object.size(df)) / 1024^2
  
  # Loop through each column in the dataframe
  for (col in names(df)) {
    # Skip columns that are factors (equivalent to 'object' in pandas)
    if (!is.factor(df[[col]])) {
      col_min <- min(df[[col]], na.rm = TRUE)
      col_max <- max(df[[col]], na.rm = TRUE)
      
      # Integer optimization
      if (is.integer(df[[col]])) {
        if (col_min >= .Machine$integer.min && col_max <= .Machine$integer.max) {
          df[[col]] <- as.integer(df[[col]])
        }
      } else if (is.numeric(df[[col]])) { # Floating-point optimization
        if (col_min > -.Machine$double.xmax && col_max < .Machine$double.xmax) {
          df[[col]] <- as.numeric(df[[col]])
        }
      }
    }
  }
  
  if (verbose) {
    end_mem <- sum(object.size(df)) / 1024^2
    decrease <- 100 * (start_mem - end_mem) / start_mem
    message(sprintf("Memory usage of dataframe is %.2f MB", start_mem))
    message(sprintf("Memory usage after optimization is: %.2f MB", end_mem))
    message(sprintf("Decreased by %.2f%%", decrease))
  }
  
  return(df)
}

```

Import the dataset

```{r}


setwd("/Users/bigmac/Desktop/MDA woohoo/projet ML prévision/")
data_all<-read_csv("/Users/bigmac/Desktop/MDA woohoo/projet ML prévision/optiver-trading-at-the-close/train.csv",
                   col_names =TRUE)

data0_bis <- data_all[data_all$stock_id == c(1,2), ]

par(mfrow = c(1, 1))
data_all$stock_id<-as.factor(data_all$stock_id)

head(data0_bis)
data_all




```

```{r}
count_nan <- function(group) {
  sum(is.na(group))
}

# Grouper les données par intervalles de 10 secondes dans 'seconds_in_bucket'
# et compter les valeurs NA dans chaque groupe (sauf pour 'seconds_in_bucket')
incr_nan <- data_all %>%
  mutate(bucket = cut(seconds_in_bucket, breaks = seq(-10, max(seconds_in_bucket, na.rm = TRUE) + 10, by = 10), include.lowest = FALSE)) %>%
  group_by(bucket) %>%
  summarise(across(-seconds_in_bucket, ~sum(is.na(.)))) %>%
  ungroup()  # Pour enlever le regroupement par 'bucket'

# Afficher les résultats
print(incr_nan)




# Transformation des données pour le tracé
incr_nan_long <- incr_nan %>%
  pivot_longer(-bucket, names_to = "variable", values_to = "missing_values")

# Tracer

p<-ggplot(incr_nan_long, aes(x = bucket, y = missing_values, color = variable, group = variable)) +
  geom_line() + # Tracer des lignes en groupant par 'variable'
  theme_minimal() + # Appliquer un thème minimal
  labs(title = "Progression des valeurs manquantes par intervalle de temps",
       x = "Intervalles de temps dans le seau (secondes)",
       y = "Nombre de valeurs manquantes",
       color = "Variable") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1), # Rotation des étiquettes de l'axe X pour une meilleure lisibilité
        plot.title = element_text(hjust = 0.5)) # Centrer le titre du graphique

new_labels <- ifelse(seq_along(incr_nan_long$bucket) %% 2 == 0, "", seq_along(incr_nan_long$bucket))

# Appliquer les nouveaux labels à l'axe des abscisses
p + scale_x_discrete(labels = new_labels)

# Analyser les valeurs manquantes par 'date_id'
analysis_nan_date <- data_all %>%
  group_by(date_id) %>%
  summarise(across(everything(), ~sum(is.na(.))))

# Afficher les résultats
print(analysis_nan_date)




# Fondre les données pour un format long approprié pour ggplot
analysis_nan_date_long <- reshape2::melt(analysis_nan_date, id.vars = "date_id")

# Filtrer les colonnes d'intérêt
columns_of_interest <- c('imbalance_size','far_price', 'near_price', 'reference_price', 'matched_size', 'bid_price', 'ask_price')  
analysis_nan_date_filtered <- subset(analysis_nan_date_long, variable %in% columns_of_interest)

# Créer le graphique
ggplot(data = analysis_nan_date_filtered, aes(x = date_id, y = value, color = variable)) +
  geom_line() +
  labs(title = "Missing values for 'columns_of_interest' over different date_ids",
       x = "date_id",
       y = "Number of Missing Values") +
  
  theme_minimal()


# Sélectionner uniquement les colonnes d'intérêt
columns_of_interest2 <- c('imbalance_size', 'reference_price', 'matched_size', 'bid_price', 'ask_price')
analysis_nan_date_filtered2 <- analysis_nan_date[, c('date_id', columns_of_interest2)]

# Transformer les données de format large à long pour le tracé avec ggplot2
analysis_nan_date_long2 <- melt(analysis_nan_date_filtered2, id.vars = 'date_id', variable.name = 'Variable', value.name = 'Number_of_Missing_Values')

# Tracer les valeurs manquantes pour les colonnes d'intérêt par date_id
ggplot(data = analysis_nan_date_long2, aes(x = date_id, y = Number_of_Missing_Values, color = Variable)) +
  geom_line() +
  labs(title = "Missing values for selected columns over different date_ids",
       x = "Date ID",
       y = "Number of Missing Values") +
  theme_minimal()

# Filtrer les lignes où imbalance_size est égal à 55
filtered_rows <- subset(analysis_nan_date, imbalance_size == 55)

# Afficher les résultats
print(filtered_rows)




analysis_nan_date_long3 <- reshape2::melt(analysis_nan_date, id.vars = "date_id", measure.vars = c("far_price", "near_price"))

# Générer le graphique
ggplot(data = analysis_nan_date_long3, aes(x = date_id, y = value, color = variable)) +
  geom_line() +
  labs(title = "Missing values for near and far prices over different date_ids",
       x = "Date ID",
       y = "Number of Missing Values",
       color = "Price Type") +
  theme_minimal()


```

```{r}
dfclean <- data_all %>% filter(!is.na(target))

# 2. Imputer les valeurs pour 'far_price' et 'near_price' pour les secondes inférieures à 300
#dfclean <- dfclean %>%
#  mutate(far_price = ifelse(seconds_in_bucket < 300 & is.na(far_price), 0, far_price),
#         near_price = ifelse(seconds_in_bucket < 300 & is.na(near_price), 0, near_price))

# 2.0 
replace_by_cond <- function(data) {
  # Identifier les lignes où `far_price` est NA
  cond <- is.na(data$far_price)
  
  # Remplacer `far_price` en fonction de la condition
  data$far_price <- ifelse(cond, 
                           ifelse(data$imbalance_buy_sell_flag == 1, data$ask_price, 
                                  ifelse(data$imbalance_buy_sell_flag == -1, data$bid_price, 
                                         (data$bid_price + data$ask_price)/2)), 
                           data$far_price)
  
  # Remplacer `near_price` en fonction de la condition
  data$near_price <- ifelse(cond, 
                            ifelse(data$imbalance_buy_sell_flag == 1, data$ask_price, 
                                   ifelse(data$imbalance_buy_sell_flag == -1, data$bid_price, 
                                          (data$bid_price + data$ask_price)/2)), 
                            data$near_price)
  
  # Ajuster `reference_price` en fonction de la condition
  data$reference_price <- ifelse(cond, 
                                 ifelse((data$near_price > data$bid_price) & (data$near_price < data$ask_price), 
                                        data$near_price, 
                                        ifelse(data$near_price > data$ask_price, data$ask_price, data$bid_price)),
                                 data$reference_price)
  
  return(data)
}

cond <- function(data) {
  # Copie des données pour éviter de modifier l'ensemble de données original
  raw <- data
  
  # Suppression des enregistrements où 'wap' est NA
  rmv <- raw[!is.na(raw$wap), ]
  
  # Application de la fonction de remplacement conditionnel
  cond <- replace_by_cond(rmv)
  
  return(cond)
}
# Application de la fonction `cond` à l'ensemble de données `train`
# L'ensemble de données `train` a été correctement chargé et préparé en R
dfclean=cond(data_all)
pourcentage_na_apres <- sapply(dfclean, function(x) mean(is.na(x))) * 100
pourcentage_na_apres_df <- as.data.frame(pourcentage_na_apres)
# Afficher le DataFrame vertical
print(pourcentage_na_apres_df)



# 5. Calculer le pourcentage de données supprimées
pourcentage_donnees_supprimees <- (nrow(data_all) - nrow(dfclean)) / nrow(data_all) * 100
print(pourcentage_donnees_supprimees)

data_all=dfclean


```

Infos

```{r}

df_infos = data.frame(
  Characteristics = c("Number of features", "Number of stocks", "Number of days in the study", "Number of data points each day", "Time increment (seconds)", "Number of total data points per stocks"), 
  Value = c(length(colnames(data_all)), length(unique(data_all$stock_id)), length(unique(data_all$date_id)), length(unique(data_all$time_id))/length(unique(data_all$date_id)), 10, length(unique(data_all$time_id))))
  
df_infos
```

```{r}

inspect_columns <- function(df) {
  # Calcul de diverses statistiques pour chaque colonne
  unique <- sapply(df, function(col) length(unique(col)) == nrow(df))
  cardinality <- sapply(df, function(col) length(unique(col)))
  with_null <- sapply(df, function(col) any(is.na(col)))
  null_pct <- sapply(df, function(col) round(sum(is.na(col)) / nrow(df) * 100, 2))
  first_row <- sapply(df, function(col) col[1])
  random_row <- sapply(df, function(col) col[sample(nrow(df), 1)])
  last_row <- sapply(df, function(col) col[nrow(df)])
  dtype <- sapply(df, function(col) class(col)[1])
  
  # Création du dataframe résultat
  result <- data.frame(unique, cardinality, with_null, null_pct, first_row, random_row, last_row, dtype, stringsAsFactors = FALSE)
  
  # Ajustement des noms de colonnes pour la cohérence avec l'exemple Python
  names(result) <- c('unique', 'cardinality', 'with_null', 'null_pct', '1st_row', 'random_row', 'last_row', 'dtype')
  
  return(result)
}
inspect_columns(data_all)

```

Select the most meaningful stocks

```{r}
n_stocks = 200
data_all_bis = copy(data_all)

task_variances <- data_all_bis %>%
  group_by(stock_id) %>%
  summarise(variance = var(target, na.rm = FALSE)) %>%
  arrange(desc(variance))

task_mean <- data_all_bis %>%
  group_by(stock_id) %>%
  summarise(mean = mean(target, na.rm = FALSE)) %>%
  arrange(desc(abs(mean)))

df_mean_var <- inner_join(task_variances, task_mean, by="stock_id")

# Select the top n_stocks tasks with the highest variance
top_n_tasks <- head(task_variances, n_stocks)
top_stocks = c(top_n_tasks$stock_id)

data_n <-subset(data_all, stock_id %in% top_stocks)
data_n<-reduceMemUsage(data_n, verbose=1)
data_n[is.na(data_n)] <- NaN

create_data_n <- function(df) {
data_all_bis = copy(df)

task_variances <- data_all_bis %>%
  group_by(stock_id) %>%
  summarise(variance = var(target, na.rm = FALSE)) %>%
  arrange(desc(variance))

task_mean <- data_all_bis %>%
  group_by(stock_id) %>%
  summarise(mean = mean(target, na.rm = FALSE)) %>%
  arrange(desc(abs(mean)))

df_mean_var <- inner_join(task_variances, task_mean, by="stock_id")

# Select the top n_stocks tasks with the highest variance
top_n_tasks <- head(task_variances, n_stocks)
top_stocks = c(top_n_tasks$stock_id)

data_n <-subset(data_all, stock_id %in% top_stocks)
data_n<-reduceMemUsage(data_n, verbose=1)
data_n[is.na(data_n)] <- NaN
return(data_n)
}

data_n <- create_data_n(data_all)


```

```{r}
ggplot(df_mean_var, aes(x = variance, y=mean)) +
  geom_point() + 
  ggtitle(paste0("Distribution des stocks selon les caractéristiques de leur target")) +
  theme_minimal() +
  theme(plot.title = element_text(size = 11))

```

```{r}


task_variances_wap <- data_all_bis %>%
  group_by(stock_id) %>%
  summarise(variance = var(wap, na.rm = FALSE)) %>%
  arrange(desc(variance))

task_mean_wap <- data_all_bis %>%
  group_by(stock_id) %>%
  summarise(mean = mean(wap, na.rm = FALSE)) %>%
  arrange(desc(abs(mean)))

df_mean_var_wap <- inner_join(task_variances_wap, task_mean_wap, by="stock_id")


ggplot(df_mean_var_wap, aes(x = variance, y=mean)) +
  geom_point() + 
  ggtitle(paste0("Distribution des stocks selon les caractéristiques de leur wap")) +
  theme_minimal() +
  theme(plot.title = element_text(size = 11))

```

```{r}
ordinary <- c('imbalance_buy_sell_flag')
size <- c('bid_size', 'ask_size', 'imbalance_size', 'matched_size')
price1 <- c('bid_price', 'ask_price', 'wap', 'reference_price')
price2 <- c('far_price', 'near_price')
label <- 'target'


data_n <- select(data_n, time_id, stock_id, date_id, seconds_in_bucket, all_of(c(ordinary, size, price1, price2, label)))

data_n <- arrange(data_n, stock_id)
```

```{r}
df_infos = data.frame(
  Characteristics = c("Number of features", "Number of stocks", "Number of days in the study", "Number of data points each day", "Time increment (seconds)", "Number of total data points per stocks"), 
  Value = c(as.integer(length(colnames(data_n))), as.integer(length(unique(data_n$stock_id))), as.integer(length(unique(data_n$date_id))), as.integer(length(unique(data_n$time_id))/length(unique(data_n$date_id))), as.integer(10), as.integer(length(unique(data_n$time_id)))))
  
df_infos
inspect_columns(data_n)
```

```{r}


columns_to_plot <- c(ordinary, size, price1, price2, "target")

ggplot(data_n, aes(x = target, fill = stock_id)) +
  geom_density(alpha = 0.1) + 
  ggtitle(paste0("Distribution de target")) +
  
  scale_x_continuous(# Personnaliser si nécessaire
                     limits = c(-50,50))+
  theme_minimal() +
  theme(plot.title = element_text(size = 11))

ggplot(data_n, aes(x = wap, fill = stock_id)) +
  geom_density(alpha = 0.2) + 
  ggtitle(paste0("Distribution de wap")) +
  scale_x_continuous(# Personnaliser si nécessaire
                     limits = c(0.98,1.02))+
  theme_minimal() +
  theme(plot.title = element_text(size = 11))

ggplot(data_n, aes(x = reference_price, fill = stock_id)) +
  geom_density(alpha = 0.2) + 
  ggtitle(paste0("Distribution de reference_price")) +
  scale_x_continuous(# Personnaliser si nécessaire
                     limits = c(-50,50))+
  theme_minimal() +
  theme(plot.title = element_text(size = 11))

ggplot(data_n, aes(x = ask_price, fill = stock_id)) +
  geom_density(alpha = 0.2) + 
  ggtitle(paste0("Distribution de ask_price")) +
  scale_x_continuous(# Personnaliser si nécessaire
                     limits = c(0.98,1.02))+
  theme_minimal() +
  theme(plot.title = element_text(size = 11))

ggplot(data_n, aes(x = bid_price, fill = stock_id)) +
  geom_density(alpha = 0.2) + 
  ggtitle(paste0("Distribution de bid_price")) +
  scale_x_continuous(# Personnaliser si nécessaire
                     limits = c(0.98,1.02))+
  theme_minimal() +
  theme(plot.title = element_text(size = 11))

ggplot(data_n, aes(x = matched_size, fill = stock_id)) +
  geom_density(alpha = 0.2) + 
  ggtitle(paste0("Distribution de matched_size")) +
  theme_minimal() +
  theme(plot.title = element_text(size = 11))

ggplot(data_n, aes(x = imbalance_size, fill = stock_id)) +
  geom_density(alpha = 0.2) + 
  ggtitle(paste0("Distribution de imbalance_size")) +
  scale_x_continuous(# Personnaliser si nécessaire
                     limits = c(0,10))+
  theme_minimal() +
  theme(plot.title = element_text(size = 11))

ggplot(data_n, aes(x = ask_size, fill = stock_id)) +
  geom_density(alpha = 0.2) + 
  ggtitle(paste0("Distribution de ask_size")) +
  theme_minimal() +
  theme(plot.title = element_text(size = 11))

ggplot(data_n, aes(x = bid_size, fill = stock_id)) +
  geom_density(alpha = 0.2) + 
  ggtitle(paste0("Distribution de bid_size")) +
  theme_minimal() +
  theme(plot.title = element_text(size = 11))



ggplot(data_n, aes(x = imbalance_buy_sell_flag, fill = stock_id)) +
  geom_density(alpha = 0.2) + 
  ggtitle(paste0("Distribution de imbalance_buy_sell_flag")) +
  theme_minimal() +
  theme(plot.title = element_text(size = 11))

```

```{r}
plot_n_days <- function(n_days, n_length, df_plot, var, type_='p') {
  #par(mfrow=c(3,as.integer(n_length/3)))
  for (i in 1:n_length){
    df_plot_batch <- df_plot[((i-1)*(55*n_days)+1):(i*(55*n_days)+1), ]
    plot(df_plot_batch[[var]], df_plot_batch$target,pch=10,cex=0.3
         , type=type_, main=paste(c("Evolution of the target with ", var)))
  }
  
  par(mfrow=c(1,1))
  
}



```

```{r}

data_n_1 <- data_n[data_n$stock_id == 31,]

density(data_n_1$target)


plot_n_days(n_days = 30, 3, data_n_1, "wap")

plot_n_days(n_days = 30, 3, data_n_1, "time_id", 'l')
plot_n_days(n_days = 30, 3, data_n_1, "date_id")
plot_n_days(n_days = 30, 3, data_n_1, "date_id")
plot_n_days(n_days = 30, 3, data_n_1, "imbalance_size")
plot_n_days(n_days = 30, 3, data_n_1, "reference_price")
plot_n_days(n_days = 30, 3, data_n_1, "ask_price")
plot_n_days(n_days = 30, 3, data_n_1, "bid_price")
plot_n_days(n_days = 30, 3, data_n_1, "ask_size")
plot_n_days(n_days = 30, 3, data_n_1, "bid_size")
plot_n_days(n_days = 30, 3, data_n_1, "matched_size")
plot_n_days(n_days = 30, 3, data_n_1, "seconds_in_bucket")
plot_n_days(n_days = 1, 3, data_n_1, "seconds_in_bucket", type='l')

```

```{r}

# Calculate Pearson correlation
pearson <- cor(data_n[, c(ordinary, size, price1)], data_n[, label], method = "pearson")

# Calculate Spearman correlation
spearman <- cor(data_n[, c(ordinary, size, price1)], data_n[, label], method = "spearman")

# Transform correlations to data frames for plotting
pearson_df <- as.data.frame(abs(pearson), row.names = c(ordinary, size, price1))
pearson_df$Variable <- rownames(pearson_df)
pearson_df <- arrange(pearson_df,-target)

spearman_df <- as.data.frame(abs(spearman), row.names = c(ordinary, size, price1))
spearman_df$Variable <- rownames(spearman_df)
spearman_df <- arrange(spearman_df,target)

# Plotting
ggplot() + 
  geom_bar(data = pearson_df, aes(x = Variable, y = target, fill = "gray"), stat = "identity") +
  coord_flip() + 
  labs(title = "Pearson Mean", x = "", y = "Correlation") + 
  theme_minimal() +
  theme(legend.position = "none")

ggplot() + 
  geom_bar(data = spearman_df, aes(x = Variable, y = target, fill = "gray"), stat = "identity") +
  coord_flip() + 
  labs(title = "Spearman Mean", x = "", y = "Correlation") + 
  theme_minimal() +
  theme(legend.position = "none")
```

```{r}
global_stock_id_feats <- list(
  median_size = data_n %>%
    group_by(stock_id) %>%
    summarise(median_bid_size = median(bid_size, na.rm = TRUE),
              median_ask_size = median(ask_size, na.rm = TRUE)) %>%
    transmute(stock_id, median_size = median_bid_size + median_ask_size) %>%
    ungroup() %>%
    select(-stock_id),

  std_size = data_n %>%
    group_by(stock_id) %>%
    summarise(std_bid_size = sd(bid_size, na.rm = TRUE),
              std_ask_size = sd(ask_size, na.rm = TRUE)) %>%
    transmute(stock_id, std_size = std_bid_size + std_ask_size) %>%
    ungroup() %>%
    select(-stock_id),
  
  ptp_size = data_n %>%
    group_by(stock_id) %>%
    summarise(ptp_bid_size = max(bid_size, na.rm = TRUE) - min(bid_size, na.rm = TRUE)) %>%
    ungroup() %>%
    rename(ptp_size = ptp_bid_size),
  
  median_price = data_n %>%
    group_by(stock_id) %>%
    summarise(median_bid_price = median(bid_price, na.rm = TRUE),
              median_ask_price = median(ask_price, na.rm = TRUE)) %>%
    transmute(stock_id, median_price = median_bid_price + median_ask_price) %>%
    ungroup() %>%
    select(-stock_id),
  
  std_price = data_n %>%
    group_by(stock_id) %>%
    summarise(std_bid_price = sd(bid_price, na.rm = TRUE),
              std_ask_price = sd(ask_price, na.rm = TRUE)) %>%
    transmute(stock_id, std_price = std_bid_price + std_ask_price) %>%
    ungroup() %>%
    select(-stock_id),
  
  ptp_price = data_n %>%
    group_by(stock_id) %>%
    summarise(ptp_bid_price = max(bid_price, na.rm = TRUE) - min(ask_price, na.rm = TRUE)) %>%
    ungroup() %>%
    rename(ptp_price = ptp_bid_price)
)


weights_vector = c(
    0.004, 0.001, 0.002, 0.006, 0.004, 0.004, 0.002, 0.006, 0.006, 0.002, 0.002, 0.008,
    0.006, 0.002, 0.008, 0.006, 0.002, 0.006, 0.004, 0.002, 0.004, 0.001, 0.006, 0.004,
    0.002, 0.002, 0.004, 0.002, 0.004, 0.004, 0.001, 0.001, 0.002, 0.002, 0.006, 0.004,
    0.004, 0.004, 0.006, 0.002, 0.002, 0.04 , 0.002, 0.002, 0.004, 0.04 , 0.002, 0.001,
    0.006, 0.004, 0.004, 0.006, 0.001, 0.004, 0.004, 0.002, 0.006, 0.004, 0.006, 0.004,
    0.006, 0.004, 0.002, 0.001, 0.002, 0.004, 0.002, 0.008, 0.004, 0.004, 0.002, 0.004,
    0.006, 0.002, 0.004, 0.004, 0.002, 0.004, 0.004, 0.004, 0.001, 0.002, 0.002, 0.008,
    0.02 , 0.004, 0.006, 0.002, 0.02 , 0.002, 0.002, 0.006, 0.004, 0.002, 0.001, 0.02,
    0.006, 0.001, 0.002, 0.004, 0.001, 0.002, 0.006, 0.006, 0.004, 0.006, 0.001, 0.002,
    0.004, 0.006, 0.006, 0.001, 0.04 , 0.006, 0.002, 0.004, 0.002, 0.002, 0.006, 0.002,
    0.002, 0.004, 0.006, 0.006, 0.002, 0.002, 0.008, 0.006, 0.004, 0.002, 0.006, 0.002,
    0.004, 0.006, 0.002, 0.004, 0.001, 0.004, 0.002, 0.004, 0.008, 0.006, 0.008, 0.002,
    0.004, 0.002, 0.001, 0.004, 0.004, 0.004, 0.006, 0.008, 0.004, 0.001, 0.001, 0.002,
    0.006, 0.004, 0.001, 0.002, 0.006, 0.004, 0.006, 0.008, 0.002, 0.002, 0.004, 0.002,
    0.04 , 0.002, 0.002, 0.004, 0.002, 0.002, 0.006, 0.02 , 0.004, 0.002, 0.006, 0.02,
    0.001, 0.002, 0.006, 0.004, 0.006, 0.004, 0.004, 0.004, 0.004, 0.002, 0.004, 0.04,
    0.002, 0.008, 0.002, 0.004, 0.001, 0.004, 0.006, 0.004
)

weights <- setNames(as.list(weights_vector), seq_along(weights_vector) - 1)


```

# FEATURES

```{r}

compute_rolling_averages <- function(df, window_sizes) {
  df_values <- as.matrix(df)
  num_features <- ncol(df_values)
  
  # Création d'une liste pour stocker les résultats
  rolling_features <- vector("list", length(window_sizes))
  names(rolling_features) <- paste("Window", window_sizes)
  
  # Calcul parallèle des moyennes mobiles pour chaque taille de fenêtre
  cl <- makeCluster(detectCores() - 1) # Utiliser un cœur de moins que le total disponible
  clusterExport(cl, varlist = c("df_values", "window_sizes"), envir = environment())
  clusterEvalQ(cl, library(zoo))
  
  rolling_features <- parLapply(cl, window_sizes, function(window) {
    # Initialisation d'une matrice pour stocker les moyennes mobiles pour cette taille de fenêtre
    result_matrix <- matrix(NA, nrow = nrow(df_values), ncol = num_features)
    
    for (feature_idx in 1:num_features) {
      # Calcul de la moyenne mobile pour chaque caractéristique
      result_matrix[, feature_idx] <- rollapply(df_values[, feature_idx], window, mean, fill = mean(df_values[[feature_idx]]), align = "right")
    }
    
    return(result_matrix)
  })
  
  stopCluster(cl)
  
  return(rolling_features)
}

compute_rolling_averages_df <- function(df){
  # Itération sur chaque colonne de prix et assignation des résultats
prices <- c("reference_price", "far_price", "near_price", "ask_price", "bid_price", "wap")
window_sizes <- c(10,5) # Exemple de tailles de fenêtre

for (price in prices) {
  rolling_avg_features <- compute_rolling_averages(df[[price]], window_sizes)
  
  # Assignation des résultats des moyennes mobiles au DataFrame
  for (i in seq_along(window_sizes)) {
    window <- window_sizes[i]
    column_name <- paste(price, "rolling_avg", window, sep = "_")
    df[[column_name]] <- rolling_avg_features[[i]]
  }
}
return (df)
}



compute_triplet_imbalance <- function(df, price) {
  comb_indices <- combn(price, 3, simplify = FALSE)
  features_list <- mclapply(comb_indices, function(c) {
    a <- df[[c[1]]]
    b <- df[[c[2]]]
    c <- df[[c[3]]]
    
    max_val <- pmax(a, b, c)
    min_val <- pmin(a, b, c)
    mid_val <- a + b + c - max_val - min_val
    
    imbalance_feature <- ifelse(mid_val == min_val, 0, (max_val - mid_val) / (mid_val - min_val))
    return(imbalance_feature)
  }, mc.cores = detectCores())
  
  features_matrix <- do.call(cbind, features_list)
  colnames(features_matrix) <- sapply(comb_indices, function(c) paste(c, collapse = "_"), USE.NAMES = FALSE)
  return(as.data.frame(features_matrix))
}

ajout_detect_outliers <- function(data_frame) {
  # Créer un dataframe pour stocker les indicateurs d'outliers
  outliers <- data_frame
  
  # Parcourir chaque colonne du dataframe
  for(col_name in names(data_frame)) {
    # Vérifier si la colonne est numérique
    if(is.numeric(data_frame[[col_name]])) {
      # Calculer Q1, Q3 et IQR
      Q1 <- quantile(data_frame[[col_name]], 0.25, na.rm = TRUE)
      Q3 <- quantile(data_frame[[col_name]], 0.75, na.rm = TRUE)
      IQR <- Q3 - Q1
      
      # Calculer les bornes pour déterminer les outliers
      lower_bound <- Q1 - 1.5 * IQR
      upper_bound <- Q3 + 1.5 * IQR
      
      # Marquer les valeurs en dehors des bornes comme TRUE (outlier)
      outliers[[col_name]] <- data_frame[[col_name]] < lower_bound | data_frame[[col_name]] > upper_bound
    } else {
      # Pour les colonnes non numériques, marquer toutes les valeurs comme FALSE
      outliers[[col_name]] <- FALSE
    }
  }
  
  # Ajouter une colonne 'has_outlier' pour marquer les lignes avec au moins un outlier
  #outliers$has_outlier <- apply(outliers, 1, function(row) as.integer(any(row)))
  data_frame$has_outlier <- apply(outliers, 1, function(row) as.integer(any(row)))
  
  return(data_frame)
}



imbalance_features <- function(df) {
  prices <- c("reference_price", "far_price", "near_price", "ask_price", "bid_price", "wap")
  sizes <- c("matched_size", "bid_size", "ask_size", "imbalance_size")
  
  df <- df %>%
    mutate(volume = ask_size + bid_size,
           mid_price = (ask_price + bid_price) / 2,
           liquidity_imbalance = (bid_size - ask_size) / (bid_size + ask_size),
           matched_imbalance = (imbalance_size - matched_size) / (matched_size + imbalance_size),
           size_imbalance = bid_size / ask_size)
  
  
  df <- cbind(df, compute_triplet_imbalance(df, prices))
  
 
  return(df)
}



other_features_lgbm <- function(df) {
  
  prices <- c("reference_price", "far_price", "near_price", "ask_price", "bid_price", "wap")
  sizes <- c("matched_size", "bid_size", "ask_size", "imbalance_size")
  
  
  df <- df %>%
  mutate(
    non_auction_size = bid_size + ask_size,
    auction_size = imbalance_size + matched_size,
    size = auction_size + non_auction_size,
    non_auction_size1 = non_auction_size / size,
    auction_size1 = auction_size / size,
    bid_size1 = bid_size / non_auction_size,
    ask_size1 = ask_size / non_auction_size,
    bid_size2 = bid_size / size,
    ask_size2 = ask_size / size,
    imbalance_size1 = imbalance_size / auction_size,
    matched_size1 = matched_size / auction_size,
    imbalance_size2 = imbalance_size / size,
    matched_size2 = matched_size / size,
    non_auction_size_diff = bid_size1 - ask_size1,
    auction_size_diff = matched_size1 - imbalance_size1,
    size_diff = non_auction_size1 - auction_size1
  )
  df <- df %>%
    group_by(stock_id) %>%
    mutate(
           imbalance_momentum = (imbalance_size - lag(imbalance_size, 1)) / matched_size,
           price_spread = ask_price - bid_price,
           spread_intensity = price_spread - lag(price_spread),
           price_pressure = imbalance_size * price_spread,
           market_urgency = price_spread * liquidity_imbalance,
           depth_pressure = (ask_size - bid_size) * (far_price - near_price),
           spread_depth_ratio = price_spread / (bid_size + ask_size),
           mid_price_movement = as.integer(sign(dplyr::lag(mid_price, 5) - mid_price)),
           micro_price = ((bid_price * ask_size) + (ask_price * bid_size)) / (bid_size + ask_size),
           relative_spread = (ask_price - bid_price) / wap)
  
  # Calcul des features statistiques pour tous les prix et tailles
  df$all_prices_mean <- rowMeans(df[, prices], na.rm = TRUE)
  df$all_sizes_mean <- rowMeans(df[, sizes], na.rm = TRUE)
  
  # Autres features calculées similaires à l'exemple Python
  
  
  df<- df %>%
  group_by(stock_id, date_id) %>%
  mutate(
    bid_size_diff_10 = (bid_size / lag(bid_size, 10)) - 1
    # Répétez pour d'autres caractéristiques et délais comme requis
  ) %>%
  ungroup()

price1 <- c('bid_price', 'ask_price', 'reference_price') 
price2 <- c('far_price', 'near_price') 

for(p in c(price1, price2)) {
  df[[paste0(p, "1")]] <- df[[p]] / df[['wap']]
}
  df <- df %>%
    mutate(
      dow = date_id %% 5,  # Jour de la semaine
      seconds = seconds_in_bucket %% 60,
      minute = seconds_in_bucket %/% 60,
      time_to_market_close = 540 - seconds_in_bucket
    )
  
  # Faut que global_stock_id_feats est défini dans notre environnement
  for (key in names(global_stock_id_feats)) {
    df[[paste0("global_", key)]] <- df[["stock_id"]] %in% global_stock_id_feats[[key]]
  }
  
  return(df)
}



other_features_norm <- function(df) {
  
  prices <- c("reference_price", "far_price", "near_price", "ask_price", "bid_price", "wap")
  sizes <- c("matched_size", "bid_size", "ask_size", "imbalance_size")
  
  
  df <- df %>%
  mutate(
    non_auction_size = bid_size + ask_size,
    auction_size = imbalance_size + matched_size,
    size = auction_size + non_auction_size,
    non_auction_size1 = non_auction_size / size,
    auction_size1 = auction_size / size,
    bid_size1 = bid_size / non_auction_size,
    ask_size1 = ask_size / non_auction_size,
    bid_size2 = bid_size / size,
    ask_size2 = ask_size / size,
    imbalance_size1 = imbalance_size / auction_size,
    matched_size1 = matched_size / auction_size,
    imbalance_size2 = imbalance_size / size,
    matched_size2 = matched_size / size,
    non_auction_size_diff = bid_size1 - ask_size1,
    auction_size_diff = matched_size1 - imbalance_size1,
    size_diff = non_auction_size1 - auction_size1
  )
  df <- df %>%
    group_by(stock_id) %>%
    mutate(
           
           price_spread = ask_price - bid_price,
           spread_depth_ratio = price_spread / (bid_size + ask_size),
           micro_price = ((bid_price * ask_size) + (ask_price * bid_size)) / (bid_size + ask_size))
           
  
  # Calcul des features statistiques pour tous les prix et tailles
  df$all_prices_mean <- rowMeans(df[, prices], na.rm = TRUE)
  df$all_sizes_mean <- rowMeans(df[, sizes], na.rm = TRUE)
  
  # Autres features calculées similaires à l'exemple Python
  
  


price1 <- c('bid_price', 'ask_price', 'reference_price') 
price2 <- c('far_price', 'near_price') 

for(p in c(price1, price2)) {
  df[[paste0(p, "1")]] <- df[[p]] / df[['wap']]
}
  df <- df %>%
    mutate(
      day = date_id %% 5,  # Jour de la semaine
      seconds = seconds_in_bucket %% 60,
      minute = seconds_in_bucket %/% 60,
      time_to_market_close = 540 - seconds_in_bucket
    )
  
  for (key in names(global_stock_id_feats)) {
    df[[paste0("global_", key)]] <- df[["stock_id"]] %in% global_stock_id_feats[[key]]
  }
  
  return(df)
}






generate_all_features <- function(df) {
  
  df <- ajout_detect_outliers(df)
  df <- compute_rolling_averages_df(df)
  df <- imbalance_features(df)
  df <- other_features_norm(df)
 
  
  return (df)
}


data_n <- generate_all_features(data_n)
all_features <- setdiff(colnames(data_n), c("row_id", "target", "time_id", "date_id"))

```

```{r}

data_n$stock_id <- as.numeric(data_n$stock_id)

# Calculate Pearson correlation
pearson <- cor(data_n[,all_features], data_n[, label], method = "pearson")

pearson_df <- as.data.frame(abs(pearson), row.names = all_features)
pearson_df$Variable <- rownames(pearson_df)
pearson_df <- arrange(pearson_df,-target)[1:20,]


# Plotting
ggplot() + 
  geom_bar(data = pearson_df, aes(x = Variable, y = target, fill = "gray"), stat = "identity") +
  coord_flip() + 
  labs(title = "Pearson Mean", x = "", y = "Correlation") + 
  theme_minimal() +
  theme(legend.position = "none")

```

```{r}


acf(data_n$target,lag.max=52)
pacf(data_n$target,lag.max=52)


```

```{r}
plot_two_ts <- function(n_days, n_length, df_plot, var1, var2, type_){
  #par(mfrow=c(3,as.integer(n_length/3)))
  for (i in 1:n_length){
    df_plot_batch <- df_plot[((i-1)*(55*n_days)+1):(i*(55*n_days)+1), ]
    plot(df_plot_batch$time_id,df_plot_batch[[var1]], type='l',xlab='Time',ylab='')
    par(new=TRUE)
    plot(df_plot_batch$time_id,df_plot_batch[[var2]],col='red',type=type_, main=paste(c(var1," and ",var2)),axes=F,xlab='',ylab='')
    axis(side = 4, col='red')
    mtext(side = 4, line = 3, var2, col='red')
    legend("topleft",c(var1,var2),col=c("black","red"),lty=1,ncol=1,bty="n")
    
  }
  par(mfrow=c(1,1))
}
```

```{r}
plot_two_ts(3, 3, data_n, "non_auction_size_diff", "target", 'l')
plot_two_ts(3, 3, data_n, "bid_size1", "target", 'l')
plot_two_ts(3, 3, data_n, "ask_size1", "target", 'l')
plot_two_ts(3, 3, data_n, "volume", "target", 'l')
plot_two_ts(3, 3, data_n, "non_auction_size", "target", 'l')
plot_two_ts(3, 3, data_n, "seconds", "target", 'l')
plot_two_ts(3, 3, data_n, "near_price_ask_price_wap", "target", 'l')
```

```{r}
plot_two_ts(3, 3, data_n, "wap_rolling_avg_5", "target", 'l')
plot_two_ts(3, 3, data_n, "wap", "target", 'l')

```

# FURTHER PREPROCESSING

## OUTLIER DETECTION

# MODELS

## USING TIME T-H

```{r}
time_id_t <- 22000

data_n$stock_id <- as.numeric(data_n$stock_id)

df_train <- data_n%>%
  group_by(stock_id)%>%
  slice_head(n=time_id_t)%>%
  ungroup()

df_test <- data_n%>%
  group_by(stock_id)%>%
  slice_tail(n=time_id_t)%>%
  ungroup()

X_train <- df_train[, all_features]
y_train <- df_train[["target"]]

X_test <- df_test[, all_features]
y_test <- df_test[["target"]]
```

```{r}
library(forecast)
library(purrr)
library(rugarch)

h_=60
all_features_to_predict <- setdiff(colnames(data_all[, sapply(data_all, is.numeric)]), c("stock_id", "seconds_in_bucket", "date_id", "row_id", "time_id", "target"))
data_to_predict <- df_test %>%
  group_by(stock_id) %>%
  slice_head(n = h_) %>%
  ungroup()%>%
  select(c(all_features_to_predict, c("stock_id", "seconds_in_bucket", "date_id", "time_id", "target")))

y_true <- data_to_predict$target

for (stock in unique(df_test$stock_id)) {
 print(stock)
for (features in all_features_to_predict){
  print(features)
  # spec <- ugarchspec(variance.model = list(model = "sGARCH"), 
  #                    mean.model = list(armaOrder = c(1, 1)),
  #                    distribution.model = "norm")
  # model_garch <- ugarchfit(spec, data = df_train[[features]])
  # forecast <- ugarchforecast(model_garch, n.ahead = h_)
  # data_to_predict[[features]][data_to_predict$stock_id==stock] <- forecast_garch
  
  
  feature_model <- auto.arima(df_train[df_train$stock_id==stock,][[features]], seasonal = TRUE) 
  forecast <- c(forecast(feature_model, h=h_)$mean)
  

  # holt_model <- holt(df_train[[features]], h=h_)
  # forecast <- forecast(holt_model, h = h_)
  # forecast<-data.frame(forecast)[["Point.Forecast"]]
  
  
  # ses_model <- ses(df_train[[features]], alpha = 0.2, h=h_) 
  # forecast <- forecast(ses_model, h = h_)
  # forecast<-data.frame(forecast)[["Point.Forecast"]]
  
  forecast <- c(forecast - (forecast[1]-df_train[df_train$stock_id==stock,][[features]][length(df_train[df_train$stock_id==stock,][[features]])]))
  data_to_predict[[features]][data_to_predict$stock_id==stock] <-forecast
  
  }
}
```

```{r}
data_to_predict <- generate_all_features(data_to_predict)

```

```{r}
data_predicted <- df_train %>%
  group_by(stock_id) %>%
  slice_tail(n = h_) %>%
  ungroup() 

data_true <- df_test %>%
  group_by(stock_id) %>%
  slice_head(n = h_) %>%
  ungroup() 

data_predicted<- full_join(data_predicted, data_true)
data_to_plot<- data_to_predict
data_predicted$predicted = rep("known", times=2*n_stocks*h_)
data_to_plot$predicted = rep("pred", times=n_stocks*h_)

data_plot <- full_join(data_predicted, data_to_plot)
one_stock_one <- 1:h_
one_stock_two = (h_+1):(2*h_)
time_id<-c(rep(one_stock_one, times=n_stocks), rep(one_stock_two, times=n_stocks), rep(one_stock_two, times=n_stocks))
data_plot$time_id = time_id
           

data_plot$time_category <- ifelse(data_plot$time_id <= h_, "Before", "After")
data_plot$time_category <- ifelse(data_plot$predicted=="pred", "Predicted", data_plot$time_category)



```

```{r}
ggplot(data_plot[data_plot$stock_id==32, ], aes(x = time_id, y = wap, color = time_category)) + 
  geom_line() +  # Draw lines
  geom_point() +  # Optionally add points
  scale_color_manual(values = c("Before" = "blue", "After" = "red", "Predicted" = "darkgreen")) +  # Set custom colors
  theme_minimal() +  # Minimal theme for clarity
  labs(x = "Time", y = "wap", title = "feature wap predicted for stock 32") + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Improve readability of x-axis labels

ggplot(data_plot[data_plot$stock_id==32, ], aes(x = time_id, y = ask_size1, color = time_category)) + 
  geom_line() +  # Draw lines
  geom_point() +  # Optionally add points
  scale_color_manual(values = c("Before" = "blue", "After" = "red", "Predicted" = "darkgreen")) +  # Set custom colors
  theme_minimal() +  # Minimal theme for clarity
  labs(x = "Time", y = "wap", title = "feature ask_size1 predicted for stock 32") + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Improve readability of x-axis labels



ggplot(data_plot[data_plot$stock_id==83, ], aes(x = time_id, y = wap, color = time_category)) + 
  geom_line() +  # Draw lines
  geom_point() +  # Optionally add points
  scale_color_manual(values = c("Before" = "blue", "After" = "red", "Predicted" = "darkgreen")) +  # Set custom colors
  theme_minimal() +  # Minimal theme for clarity
  labs(x = "Time", y = "wap", title = "feature wap predicted for stock 83") + 

  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Improve readability of x-axis labels


ggplot(data_plot[data_plot$stock_id==83, ], aes(x = time_id, y = wap_rolling_avg_5, color = time_category)) + 
  geom_line() +  # Draw lines
  geom_point() +  # Optionally add points
  scale_color_manual(values = c("Before" = "blue", "After" = "red", "Predicted" = "darkgreen")) +  # Set custom colors
  theme_minimal() +  # Minimal theme for clarity
    labs(x = "Time", y = "wap rolling avg", title = "feature rolling avg wap predicted for stock 83") + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Improve readability of x-axis labels


ggplot(data_plot[data_plot$stock_id==32, ], aes(x = time_id, y = bid_size, color = time_category)) + 
  geom_line() +  # Draw lines
  geom_point() +  # Optionally add points
  scale_color_manual(values = c("Before" = "blue", "After" = "red", "Predicted" = "darkgreen")) +  # Set custom colors
  theme_minimal() +  # Minimal theme for clarity
  labs(x = "Time", y = "bid_size", title = "Time Series") + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Improve readability of x-axis labels



```

```{r}
# Initialisation d'une liste pour stocker les modèles
models <- list()

# Initialisation d'une liste pour stocker les erreurs MAE de chaque modèle
mae_errors_pred <- numeric(47)
y_pred_short <- c()
y_true <- c()
times <- c()
# Boucle sur les périodes de 10 jours, pour un total de 47 itérations
for (i in 0:46) {
  print(i)
  # Définition des intervalles de jours pour l'entraînement et le test
  train_start_day <- i * 10
  test_start_day <- train_start_day + 10
  
  # Sélection des données d'entraînement et de test
  train_data <- filter(data_n, date_id > train_start_day & date_id <= train_start_day + 10)
  test_data <- filter(data_n, date_id > test_start_day & date_id <= test_start_day + 10)
  
  all_features_init <- setdiff(colnames(data_all[, sapply(data_all, is.numeric)]), c("target", "row_id"))
  # Séparation des caractéristiques et de la cible
  train_x <- select(train_data, -target)
  train_y <- train_data$target
  test_x <- select(test_data, -target)
  test_y <- test_data$target
  
  all_features_to_predict <- setdiff(colnames(data_all[, sapply(data_all, is.numeric)]), c("stock_id", "seconds_in_bucket", "date_id", "row_id", "time_id", "target"))


for (stock in unique(train_x$stock_id)) {
 print(stock)
for (features in all_features_to_predict){
  
  # spec <- ugarchspec(variance.model = list(model = "sGARCH"), 
  #                    mean.model = list(armaOrder = c(1, 1)),
  #                    distribution.model = "norm")
  # model_garch <- ugarchfit(spec, data = df_train[df_train$stock_id==stock,][[features]])
  # forecast <- ugarchforecast(model_garch, n.ahead = h_)
  # data_to_predict[[features]][data_to_predict$stock_id==stock] <- forecast_garch
  
  
  feature_model <- auto.arima(train_x[train_x$stock_id==stock,][[features]], seasonal = TRUE) 
  forecast <- c(forecast(feature_model, h=h_)$mean)
  

  # holt_model <- holt(df_train[df_train$stock_id==stock,][[features]], h=h_)
  # forecast <- forecast(holt_model, h = h_)
  # forecast<-data.frame(forecast)[["Point.Forecast"]]
  
  
  # ses_model <- ses(train_x[train_x$stock_id==stock,][[features]], alpha = 0.2) 
  # forecast <- forecast(ses_model, h = length(test_y))
  # forecast<-data.frame(forecast)[["Point.Forecast"]]
  
  test_x[[features]][test_x$stock_id==stock] <-forecast
  
  }
}
  test_x <- generate_all_features(test_x)
  ####### Préparation des données pour LightGBM
  dtrain <- lgb.Dataset(data = as.matrix(train_x), label = train_y)
  dtest <- lgb.Dataset(data = as.matrix(test_x), label = test_y)
  
  # Configuration des paramètres de LightGBM
  params <- list(
    objective = "regression_l1",  # MAE
    metric = "mae",
    num_leaves = 31,
    learning_rate = 0.05,
    n_estimators = 60
  )
  
  # Entraînement du modèle LightGBM
  lightgbm_model <- lgb.train(params, dtrain, valids = list(test = dtest), verbose = 1)
  
  # Stockage du modèle
  models[[i + 1]] <- lightgbm_model
  
  # Prédiction avec le modèle LightGBM
  lightgbm_predictions <- predict(lightgbm_model, as.matrix(test_x))
  clipped_predictions <- pred_boost(lightgbm_predictions)
  y_pred_short<-c(y_pred_short, clipped_predictions)
  y_true <- c(y_true, test_y)
  time<-c(time, c(test_x$time_id))
  # Calcul et stockage de l'erreur MAE
  mae_errors_pred[i + 1] <- mean(abs(test_y - clipped_predictions))
}

# Affichage des erreurs MAE
print(mae_errors_pred)

# Examiner les erreurs MAE pour évaluer la performance de chaque modèle sur son ensemble de test correspondant.
mean(mae_errors_pred)
```

## SOTA MODELS

```{r}
data_n$stock_id <- as.numeric(data_n$stock_id)
features_init <- setdiff(colnames(data_all), "row_id")

df_train <- data_n[data_n$date_id <= 420, features_init]
df_test <- data_n[data_n$date_id > 420, features_init]

train_y <- df_train$target
train_x <- df_train[, colnames(df_train) != "target"]
test_y <- df_test$target
test_x <- df_test[, colnames(df_test) != "target"]

train_x <- train_x[, !(names(train_x) %in% c("row_id"))]
test_x <- test_x[, !(names(test_x) %in% c("row_id"))]

```

```{r}
zero_sum <- function(prices, volumes) {
  std_error <- sqrt(volumes)
  step <- sum(prices) / sum(std_error)
  out <- prices - std_error * step
  return(out)
}

pred_boost <- function(pred){
  pred_adjusted <- pred - mean(pred)
  y_min <- -64
  y_max <- 64
  clipped_predictions <- pmin(pmax(pred_adjusted, y_min), y_max)
  return(clipped_predictions)
}

```

```{r}
####### Préparation des données pour LightGBM
dtrain <- lgb.Dataset(data = as.matrix(train_x), label = train_y)
dtest <- lgb.Dataset(data = as.matrix(test_x), label = test_y)

# Configuration des paramètres de LightGBM
params <- list(
  objective = "regression_l1",  # MAE
  metric = "mae",
  num_leaves = 31,
  learning_rate = 0.05,
  n_estimators = 200
)

# Entraînement du modèle LightGBM
lightgbm_model <- lgb.train(params, dtrain, valids = list(test = dtest), verbose = 1)

# Prédiction avec le modèle LightGBM
lightgbm_predictions <- predict(lightgbm_model, as.matrix(test_x))

clipped_predictions <- pred_boost(lightgbm_predictions)

# clipped_predictions now contains your adjusted and clipped predictions

# Calcul de MAE pour LightGBM
lightgbm_mae <- mean(abs(test_y - clipped_predictions))
```

```{r}
print(paste("MAE pour LightGBM:", lightgbm_mae))
```

```{r}
data_n_ <- create_data_n(data_all)
data_n_$stock_id <- as.numeric(data_n_gam$stock_id)
data_n_ <- ajout_detect_outliers(data_n_)

df_train <- data_n_[data_n_$date_id <= 420, ]
df_test <- data_n_[data_n_$date_id > 420, ]

train_y <- df_train$target
train_x <- df_train[, colnames(df_train) != "target"]
test_y <- df_test$target
test_x <- df_test[, colnames(df_test) != "target"]

train_x <- train_x[, !(names(train_x) %in% c("row_id"))]
test_x <- test_x[, !(names(test_x) %in% c("row_id"))]

```

```{r}
####### Préparation des données pour LightGBM
dtrain <- lgb.Dataset(data = as.matrix(train_x), label = train_y)
dtest <- lgb.Dataset(data = as.matrix(test_x), label = test_y)

# Configuration des paramètres de LightGBM
params <- list(
  objective = "regression_l1",  # MAE
  metric = "mae",
  num_leaves = 31,
  learning_rate = 0.05,
  n_estimators = 200
)

# Entraînement du modèle LightGBM
lightgbm_model <- lgb.train(params, dtrain, valids = list(test = dtest), verbose = 1)

# Prédiction avec le modèle LightGBM
lightgbm_predictions <- predict(lightgbm_model, as.matrix(test_x))

clipped_predictions <- pred_boost(lightgbm_predictions)

# clipped_predictions now contains your adjusted and clipped predictions

# Calcul de MAE pour LightGBM
lightgbm_mae <- mean(abs(test_y - clipped_predictions))
```

#GAM

```{r}
data_n_gam <- create_data_n(data_all)
data_n_gam <- generate_all_features(data_n_gam)
data_n_gam$stock_id <- as.numeric(data_n_gam$stock_id)

models_gam <- list()

# Initialisation d'une liste pour stocker les erreurs MAE de chaque modèle
mae_errors <- numeric(47)
y_pred_short <- c()
y_true <- c()
times <- c()
# Boucle sur les périodes de 10 jours, pour un total de 47 itérations
for (i in 0:46) {
  print(i)
  # Définition des intervalles de jours pour l'entraînement et le test
  train_start_day <- i * 10
  test_start_day <- train_start_day + 10
  
  # Sélection des données d'entraînement et de test
  train_data <- filter(data_n_gam, date_id > train_start_day & date_id <= train_start_day + 10)
  test_data <- filter(data_n_gam, date_id > test_start_day & date_id <= test_start_day + 10)
  
  # Séparation des caractéristiques et de la cible
  train_x <- select(train_data, -target)
  train_y <- train_data$target
  test_x <- select(test_data, -target)
  test_y <- test_data$target
  
  #formula_string <- as.formula(paste("target ~", paste("s(", features_init, ", k=10)", collapse = " + ")))
  formula_string <- target ~ s(ask_price1, k=10) + s(bid_price1, k=10) + s(seconds_in_bucket, k=10) + s(auction_size, k=8) + s(matched_size, k=8) + s(liquidity_imbalance, k=8) + s(price_spread, k=8)
  gam1<-gam(formula_string, data=train_data, select=TRUE)
  # Stockage du modèle
  models_gam[[i + 1]] <- gam1
  
  
  # Prédiction avec le modèle LightGBM
  forecast<-predict(gam1, newdata=test_x) 
  clipped_predictions <- pred_boost(forecast)
  y_pred_short<-c(y_pred_short, clipped_predictions)
  y_true <- c(y_true, test_y)
  time<-c(time, c(test_x$time_id))
  # Calcul et stockage de l'erreur MAE
  mae_errors[i + 1] <- mean(abs(test_y - clipped_predictions))
}

# Affichage des erreurs MAE
print(mae_errors)

# Examiner les erreurs MAE pour évaluer la performance de chaque modèle sur son ensemble de test correspondant.
mean(mae_errors)
```

```{r}


```

```{r}
for (model_gam in models_gam){

  fit1 <- getViz(model_gam, nsim = 50)
plot(sm(fit1, 1), n = 400) + l_points() + l_fitLine() + l_ciLine()
( check1D(fit1, "ask_price1") + l_gridCheck1D(gridFun = mean, stand = "sc", n=100) )$ggObj
  
}
```

```{r}
data_n_gam <- create_data_n(data_all)
data_n_gam$stock_id <- as.numeric(data_n_gam$stock_id)

df_train <- data_n_gam[data_n_gam$date_id <= 420, features_init]
df_test <- data_n_gam[data_n_gam$date_id > 420, features_init]

train_y <- df_train$target
train_x <- df_train[, colnames(df_train) != "target"]
test_y <- df_test$target
test_x <- df_test[, colnames(df_test) != "target"]

train_x <- train_x[, !(names(train_x) %in% c("row_id"))]
test_x <- test_x[, !(names(test_x) %in% c("row_id"))]

```

```{r}
formula_string <- target ~ s(stock_id, k=5) + s(seconds_in_bucket, k=10) + s(wap, k=5) + s(ask_size, k=8) + s(bid_size, k=8) + s(bid_price, k=8) + s(ask_price, k=8)
gam1<-gam(formula_string, data=df_train, select=TRUE)
forecast<-predict(gam1, newdata=test_x) 
clipped_predictions <- pred_boost(forecast)
mean(abs(test_y - clipped_predictions))


# Affichage des erreurs MAE
print(mae_errors)

# Examiner les erreurs MAE pour évaluer la performance de chaque modèle sur son ensemble de test correspondant.
mean(mae_errors)
```

#STOCKS

```{r}
mae_list_stocks <- c()

for (stock in unique(data_n$stock_id)){
  df_train <- data_n[data_n$date_id <= 420 & data_n$stock_id==stock, ]
df_test <- data_n[data_n$date_id > 420& data_n$stock_id==stock, ]

train_y <- df_train$target
train_x <- df_train[, colnames(df_train) != "target"]
test_y <- df_test$target
test_x <- df_test[, colnames(df_test) != "target"]

train_x <- train_x[, !(names(train_x) %in% c("row_id"))]
test_x <- test_x[, !(names(test_x) %in% c("row_id"))]
  dtrain <- lgb.Dataset(data = as.matrix(train_x), label = train_y)
dtest <- lgb.Dataset(data = as.matrix(test_x), label = test_y)

# Configuration des paramètres de LightGBM
params <- list(
  objective = "regression_l1",  # MAE
  metric = "mae",
  num_leaves = 31,
  learning_rate = 0.05,
  n_estimators = 50
)

# Entraînement du modèle LightGBM
lightgbm_model <- lgb.train(params, dtrain, valids = list(test = dtest), verbose = 1)

# Prédiction avec le modèle LightGBM
lightgbm_predictions <- predict(lightgbm_model, as.matrix(test_x))

clipped_predictions <- pred_boost(lightgbm_predictions)

# clipped_predictions now contains your adjusted and clipped predictions

# Calcul de MAE pour LightGBM
lightgbm_mae <- mean(abs(test_y - clipped_predictions))
mae_list_stocks<-c(mae_list_stocks, lightgbm_mae)
}

mae_list_stocks

```

```{r}
final_summary_df <- data.frame(
  stock_id = 0:199,  # Assuming stock_id ranges from 0 to 199
  Total_Outliers = c(15588, 19967, 18556, 26253, 14474, 22570, 16071, 39101, 36835, 13814,
        11761, 40159, 32952, 23816, 46438, 22235, 16843, 36167, 29335, 21728,
        14715, 10618, 34154, 20836, 11106, 14996, 56123, 19398, 16540, 19515,
        12078, 51433, 11669, 21725, 23710, 14023, 23287, 22400, 26968, 13943,
        21519, 68855, 25332, 11219, 16089, 64138, 18040, 11213, 19188, 21901,
        21242, 28467, 12807, 21427, 20509, 15178, 23687, 18099, 36328, 25212,
        23803, 35350, 22587, 12112, 16829, 28737, 12585, 32727, 14087, 54736,
        36276, 24498, 34687, 22024, 20298, 18915, 10185, 20076, 33678, 28917,
        32373, 19692, 58792, 84270, 64603, 40989, 50631, 28473, 44548, 26113,
        12635, 23281, 40564, 15237, 21008, 61494, 37663, 15646, 39514, 23457,
        38170, 30298, 11481, 21610, 18611, 31192, 13739, 22929, 25057, 47970,
        32003, 23864, 72610, 17095, 16070, 19852, 13069, 48751, 33316, 19031,
        14916, 34337, 53626, 28669, 20589, 21091, 43539, 31290, 17763, 38601,
        24804, 13401, 12105, 22340, 40237, 24197, 11469, 13525, 20563, 18174,
        46025, 39850, 55338, 45780, 14596, 18718, 16156, 26368, 14881, 13347,
        23706, 69446, 15106, 21902, 17241, 17929, 40803, 18433, 34963, 19464,
        41849, 37987, 25000, 40750, 11905, 34939, 28816, 10833, 72032, 12881,
        11474, 13973, 26847, 16454, 49849, 65924, 23879, 32118, 21585, 58851,
        31135, 12186, 17006, 31686, 24614, 17239, 17431, 13760, 29188, 13165,
        21623, 74294, 11978, 40960, 19834, 14315, 11769, 17602, 65469, 20466)  
)

final_summary_df$score <- mae_list_stocks

plot(x=final_summary_df$Total_Outliers, y=final_summary_df$score, xlab='Outlier count', ylab = 'score')


```

```{r}
model <- lm(score ~ Total_Outliers, data = final_summary_df)
plot(x=final_summary_df$Total_Outliers, y=final_summary_df$score, xlab='Outlier count', ylab = 'score')
abline(model, col = "red") # 
# Get a summary of the model
model_summary <- summary(model)
model
plot(model)


# The summary includes the R-squared value

```

```{r}
print(paste("MAE pour LightGBM:", mae_list_stocks))

```

```{r}
best_stocks <- order(mae_list_stocks)[1:40]



task_variances_wap <- data_all %>%
  group_by(stock_id) %>%
  summarise(variance = var(wap, na.rm = FALSE)) %>%
  arrange(desc(variance))

task_mean_wap <- data_all%>%
  group_by(stock_id) %>%
  summarise(mean = mean(wap, na.rm = FALSE)) %>%
  arrange(desc(abs(mean)))

df_mean_var_wap <- inner_join(task_variances_wap, task_mean_wap, by="stock_id")
df_mean_var_wap <- df_mean_var_wap %>%
  mutate(isgood= ifelse(stock_id %in% best_stocks, 1, 0))


ggplot(df_mean_var_wap, aes(x = variance, y=mean, color =isgood)) +
  geom_point() + 
  ggtitle(paste0("Distribution des stocks selon les caractéristiques de leur wap")) +
  theme_minimal() +
  theme(plot.title = element_text(size = 11))

```

```{r}
best_stocks <- order(mae_list_stocks)[1:20]



task_variances_target <- data_all %>%
  group_by(stock_id) %>%
  summarise(variance = var(target, na.rm = FALSE)) %>%
  arrange(desc(variance))

task_mean_target <- data_all%>%
  group_by(stock_id) %>%
  summarise(mean = mean(target, na.rm = FALSE)) %>%
  arrange(desc(abs(mean)))

df_mean_var_target <- inner_join(task_variances_target, task_mean_target, by="stock_id")
df_mean_var_target <- df_mean_var_target %>%
  mutate(isgood= ifelse(stock_id %in% best_stocks, 1, 0))


ggplot(df_mean_var_target, aes(x = variance, y=mean, color =isgood)) +
  geom_point() + 
  ggtitle(paste0("Distribution des stocks selon les caractéristiques de leur target")) +
  theme_minimal() +
  theme(plot.title = element_text(size = 11))

```

```{r}

hist(mae_list_stocks, breaks = 1:100*0.3)

df<- data.frame(mae=mae_list_stocks)



ggplot(df, aes(x = mae)) +
  geom_density(alpha = 0.2) + 
  ggtitle(paste0("Distribution de la mae des stocks")) +
  theme_minimal() +
  theme(plot.title = element_text(size = 11))
```

```{r}
worst_stocks <- order(mae_list_stocks)[(length(mae_list_stocks)-40):length(mae_list_stocks)]


df_train <- data_n[data_n$date_id <= 420, features_init]
df_test <- data_n[data_n$date_id > 420, features_init]

df_train <- df_train[!df_train$stock_id %in% worst_stocks, ]
df_test <- df_test[!df_test$stock_id %in% worst_stocks, ]

train_y <- df_train$target
train_x <- df_train[, colnames(df_train) != "target"]
test_y <- df_test$target
test_x <- df_test[, colnames(df_test) != "target"]

train_x <- train_x[, !(names(train_x) %in% c("row_id"))]
test_x <- test_x[, !(names(test_x) %in% c("row_id"))]


####### Préparation des données pour LightGBM
dtrain <- lgb.Dataset(data = as.matrix(train_x), label = train_y)
dtest <- lgb.Dataset(data = as.matrix(test_x), label = test_y)

# Configuration des paramètres de LightGBM
params <- list(
  objective = "regression_l1",  # MAE
  metric = "mae",
  num_leaves = 31,
  learning_rate = 0.05,
  n_estimators = 200
)

# Entraînement du modèle LightGBM
lightgbm_model <- lgb.train(params, dtrain, valids = list(test = dtest), verbose = 1)

# Prédiction avec le modèle LightGBM
lightgbm_predictions <- predict(lightgbm_model, as.matrix(test_x))

clipped_predictions <- pred_boost(lightgbm_predictions)

# clipped_predictions now contains your adjusted and clipped predictions

# Calcul de MAE pour LightGBM
lightgbm_mae <- mean(abs(test_y - clipped_predictions))
```

```{r}
print(paste("MAE pour LightGBM without worst stocks:", lightgbm_mae))
```

```{r}

mae_two_models <- c()

for (n_worst in 1:100){
  
n_best <- 200-n_worst
worst_stocks <- order(mae_list_stocks)[(length(mae_list_stocks)-n_worst):length(mae_list_stocks)]
best_stocks <- order(mae_list_stocks)[1:n_best]


df_train <- data_n[data_n$date_id <= 420, features_init]
df_test <- data_n[data_n$date_id > 420, features_init]



df_train <- df_train[!df_train$stock_id %in% worst_stocks, ]
df_test <- df_test[!df_test$stock_id %in% worst_stocks, ]
train_y <- df_train$target
train_x <- df_train[, colnames(df_train) != "target"]
test_y <- df_test$target
test_x <- df_test[, colnames(df_test) != "target"]
train_x <- train_x[, !(names(train_x) %in% c("row_id"))]
test_x <- test_x[, !(names(test_x) %in% c("row_id"))]
dtrain <- lgb.Dataset(data = as.matrix(train_x), label = train_y)
dtest <- lgb.Dataset(data = as.matrix(test_x), label = test_y)
params <- list(
  objective = "regression_l1",  # MAE
  metric = "mae",
  num_leaves = 31,
  learning_rate = 0.05,
  n_estimators = 200
)
lightgbm_model_best <- lgb.train(params, dtrain, valids = list(test = dtest), verbose = 0)
lightgbm_predictions <- predict(lightgbm_model_best, as.matrix(test_x))
clipped_predictions <- pred_boost(lightgbm_predictions)
lightgbm_mae_best <- mean(abs(test_y - clipped_predictions))



df_train <- data_n[data_n$date_id <= 420, features_init]
df_test <- data_n[data_n$date_id > 420, features_init]

df_train <- df_train[df_train$stock_id %in% worst_stocks, ]
df_test <- df_test[df_test$stock_id %in% worst_stocks, ]
train_y <- df_train$target
train_x <- df_train[, colnames(df_train) != "target"]
test_y <- df_test$target
test_x <- df_test[, colnames(df_test) != "target"]
train_x <- train_x[, !(names(train_x) %in% c("row_id"))]
test_x <- test_x[, !(names(test_x) %in% c("row_id"))]
dtrain <- lgb.Dataset(data = as.matrix(train_x), label = train_y)
dtest <- lgb.Dataset(data = as.matrix(test_x), label = test_y)
params <- list(
  objective = "regression_l1",  # MAE
  metric = "mae",
  num_leaves = 31,
  learning_rate = 0.05,
  n_estimators = 200
)
lightgbm_model_worst <- lgb.train(params, dtrain, valids = list(test = dtest), verbose = 0)
lightgbm_predictions <- predict(lightgbm_model_worst, as.matrix(test_x))
clipped_predictions <- pred_boost(lightgbm_predictions)
lightgbm_mae_worst <- mean(abs(test_y - clipped_predictions))


mae_tot <- n_worst/200*lightgbm_mae_worst + n_best/200*lightgbm_mae_best

print(n_worst)
print(mae_tot)

mae_two_models<-c(mae_two_models, mae_tot)
}

```

```{r}
plot(1:100, mae_two_models)
abline(h=5.74)
```

```{r}
####### XGBoost
dtrain <- xgb.DMatrix(data = as.matrix(train_x), label = train_y)
dtest <- xgb.DMatrix(data = as.matrix(test_x), label = test_y)
params <- list(objective = "reg:squarederror", eval_metric = "mae")
xgb_model <- xgb.train(params = params, data = dtrain, nrounds = 100, verbose=1)
xgb_predictions <- predict(xgb_model, dtest)
clipped_predictions <- pred_boost(xgb_predictions)
xgb_mae <- mean(abs(test_y - clipped_predictions))
```

```{r}
# Affichage des erreurs MAE
print(paste("XGBoost MAE:", xgb_mae))
```

```{r}

data_n<- create_data_n(data_all)
data_n$stock_id <- as.numeric(data_n$stock_id)
data_n <- generate_all_features(data_n)
df_train <- data_n[data_n$date_id <= 420, ]
df_test <- data_n[data_n$date_id > 420, ]

train_y <- df_train$target
train_x <- df_train[, colnames(df_train) != "target"]
test_y <- df_test$target
test_x <- df_test[, colnames(df_test) != "target"]

train_x <- train_x[, !(names(train_x) %in% c("row_id"))]
test_x <- test_x[, !(names(test_x) %in% c("row_id"))]
```

```{r}
# Initialisation d'une liste pour stocker les modèles
models <- list()

# Initialisation d'une liste pour stocker les erreurs MAE de chaque modèle
mae_errors <- numeric(47)
y_pred_short <- c()
y_true <- c()
times <- c()
# Boucle sur les périodes de 10 jours, pour un total de 47 itérations
for (i in 0:46) {
  # Définition des intervalles de jours pour l'entraînement et le test
  train_start_day <- i * 10
  test_start_day <- train_start_day + 10
  
  # Sélection des données d'entraînement et de test
  train_data <- filter(data_n, date_id > train_start_day & date_id <= train_start_day + 10)
  test_data <- filter(data_n, date_id > test_start_day & date_id <= test_start_day + 10)
  
  # Séparation des caractéristiques et de la cible
  train_x <- select(train_data, -target)
  train_y <- train_data$target
  test_x <- select(test_data, -target)
  test_y <- test_data$target
  
  ####### Préparation des données pour LightGBM
  dtrain <- lgb.Dataset(data = as.matrix(train_x), label = train_y)
  dtest <- lgb.Dataset(data = as.matrix(test_x), label = test_y)
  
  # Configuration des paramètres de LightGBM
  params <- list(
    objective = "regression_l1",  # MAE
    metric = "mae",
    num_leaves = 31,
    learning_rate = 0.05,
    n_estimators = 100
  )
  
  # Entraînement du modèle LightGBM
  lightgbm_model <- lgb.train(params, dtrain, valids = list(test = dtest), verbose = 1)
  
  # Stockage du modèle
  models[[i + 1]] <- lightgbm_model
  
  # Prédiction avec le modèle LightGBM
  lightgbm_predictions <- predict(lightgbm_model, as.matrix(test_x))
  clipped_predictions <- pred_boost(lightgbm_predictions)
  y_pred_short<-c(y_pred_short, clipped_predictions)
  y_true <- c(y_true, test_y)
  time<-c(time, c(test_x$time_id))
  # Calcul et stockage de l'erreur MAE
  mae_errors[i + 1] <- mean(abs(test_y - clipped_predictions))
}

# Affichage des erreurs MAE
print(mae_errors)

# Examiner les erreurs MAE pour évaluer la performance de chaque modèle sur son ensemble de test correspondant.
mean(mae_errors)
```

```{r}
time_ = (10*55):(7*10*55)

for (i in 1:5) {
  train_start_time <- i * 2*55
  test_start_time <- train_start_time + 2*55
  test_stop_time <- test_start_time + 2*55-1
  
  y1 <-  y_true[(time_[1]+test_start_time-1):(time_[1]+test_stop_time-1)]
  y2 <- y_pred_short[(time_[1]+test_start_time-1):(time_[1]+test_stop_time-1)]
  plot(test_start_time:test_stop_time, y1, type = "l", col = "red", ylim = c(min(c(y1, y2)), max(c(y1, y2))), xlab = "time", ylab = "targets", main="true target vs predicted")

# Add the second time series
lines(test_start_time:test_stop_time, y2, col = "darkgreen")

# Add a legend
legend("topright", legend = c("true", "pred"), col = c("red", "darkgreen"), lty = 1)
}



```

```{r}
most_important_features <- c()
for (lgbm_model in models){
  importance_matrix_lgbm <- lgb.importance(lgbm_model)
  most_important_features<-union(most_important_features, importance_matrix_lgbm$Feature[1:5])
lgb.plot.importance(importance_matrix_lgbm)
}
most_important_features<-setdiff(most_important_features,"far_price_rolling_avg_10")
```

```{r}


# Initialisation d'une liste pour stocker les modèles
models_new <- list()

# Initialisation d'une liste pour stocker les erreurs MAE de chaque modèle
mae_errors_new <- numeric(47)

y_pred_long <- c()
y_true <- data_n$target
# Boucle sur les périodes de 10 jours, pour un total de 47 itérations
for (i in 0:46) {
  # Définition des intervalles de jours pour l'entraînement et le test
  train_end_day <- (i + 1) * 10
  test_start_day <- train_end_day + 1
  test_end_day <- test_start_day + 9
  
  # Sélection des données d'entraînement et de test
  train_data <- data_n[data_n$date_id <= train_end_day, ]
  test_data <- data_n[data_n$date_id >= test_start_day & data_n$date_id <= test_end_day, ]
  
  # Séparation des caractéristiques et de la cible
  train_x <- train_data[, setdiff(names(train_data), "target")]
  train_y <- train_data[["target"]]
  test_x <- test_data[, setdiff(names(test_data), "target")]
  test_y <- test_data[["target"]]
  
  # Préparation des données pour LightGBM
  dtrain <- lgb.Dataset(data = as.matrix(train_x), label = train_y)
  dtest <- lgb.Dataset(data = as.matrix(test_x), label = test_y)
  
  # Configuration des paramètres de LightGBM
  params <- list(
    objective = "regression_l1",  # MAE
    metric = "mae",
    num_leaves = 31,
    learning_rate = 0.05,
    n_estimators = 100
  )
  
  # Entraînement du modèle LightGBM
  lightgbm_model <- lgb.train(params, dtrain, valids = list(test = dtest), verbose = 1)
  
  # Stockage du modèle
  models_new[[i + 1]] <- lightgbm_model
  
  # Prédiction avec le modèle LightGBM
  lightgbm_predictions <- predict(lightgbm_model, as.matrix(test_x))
  clipped_predictions <- pred_boost(lightgbm_predictions)
  y_pred_long<-c(y_pred_long, lightgbm_predictions)
  # Calcul et stockage de l'erreur MAE
  mae_errors_new[i + 1] <- mean(abs(test_y - lightgbm_predictions))
}
```

```{r}
# Affichage des erreurs MAE
print(mae_errors_new)
print(mae_errors)
# Calcul de la moyenne des erreurs MAE
mean(mae_errors_new)
mean(mae_errors)

# Compare the two lists element-wise
comparison <- mae_errors >= mae_errors_new

# Convert the TRUE/FALSE values to 1/0
comparison_values <- as.integer(comparison)

# Calculate the proportion of 1's in the comparison
proportion_of_ones <- sum(comparison_values) / length(comparison_values)

# Return both the comparison list and the proportion
list(comparison_values = comparison_values, proportion_of_ones = proportion_of_ones)

difference <- mae_errors - mae_errors_new
difference
print(max(difference))
print(min(difference))
print(mean(difference))



```

```{r}
importance_matrix_xgb <- xgb.importance(model = xgb_model)
xgb.plot.importance(importance_matrix_xgb)

importance_matrix_lgbm <- lgb.importance(lgbm_model)
lgb.plot.importance(importance_matrix_lgbm)
```

# Hierarchical models

```{r}
library(caret) # Pour le training des modèles


set.seed(123) 

categories <- unique(df_train$stock_id)
models <- list()

for(cat in categories) {
  subset_df <- df_train[df_train$stock_id == cat, ]
  
  # Ici, nous utilisons une régression logistique comme exemple de modèle
  model <- train(target ~ ., data = subset_df, method = "glmnet", preProcess=c("center", "scale"))
  
  # Stockage du modèle entraîné
  models[[cat]] <- model
}

# Fonction pour prédire les labels d'un nouveau dataset
predict_labels <- function(new_data) {
  predicted_labels <- numeric(nrow(new_data))
  for(i in seq_along(predicted_labels)) {
    row <- new_data[i, ]
    cat <- row$stock_id
    model <- models[[as.character(cat)]]
    predicted_labels[i] <- predict(model, newdata = row, type = "response")
    
  }
  return(predicted_labels)
}

predicted_labels <- predict_labels(X_test)
print(paste0("mae :", mae(actual = y_test, predicted = predicted_labels)))




```
